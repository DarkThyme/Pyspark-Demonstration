{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1506575-e657-4d34-9e1c-01eb3ae0140f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ec1111-06c8-41f5-9186-f4e6ff42b9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Challenges').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b950ad-6cbd-4d53-9742-48a081a81500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DateType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756f1b47-adcf-4efd-99bf-863d392b145f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 1: Total Spend Per Customer\n",
    "\n",
    "Calculate total amount spent by each customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a60196-102d-4e6b-afb1-27208d36cf80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|customer_id|amount|\n+-----------+------+\n|          1|   250|\n|          2|   450|\n|          1|   100|\n|          3|   300|\n|          2|   150|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(customer_id=1, amount=250),\n",
    "    Row(customer_id=2, amount=450),\n",
    "    Row(customer_id=1, amount=100),\n",
    "    Row(customer_id=3, amount=300),\n",
    "    Row(customer_id=2, amount=150)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97c73e2-5c3b-4507-b9eb-4eeba2f83bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|customer_id|sum(amount)|\n+-----------+-----------+\n|          1|        350|\n|          2|        600|\n|          3|        300|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"customer_id\").agg(F.sum(\"amount\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5df2edb-7540-4934-a097-543ab1f18dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 2: Highest Transaction Per Day\n",
    "\n",
    "Find the highest transaction amount for each day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c247925c-af78-4483-9616-5926c92bd34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|      date|amount|\n+----------+------+\n|2023-01-01|   100|\n|2023-01-01|   300|\n|2023-01-02|   150|\n|2023-01-02|   200|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(date='2023-01-01', amount=100),\n",
    "    Row(date='2023-01-01', amount=300),\n",
    "    Row(date='2023-01-02', amount=150),\n",
    "    Row(date='2023-01-02', amount=200)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6524631-924f-49b9-b6cd-a63f3daa835b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|      date|max(amount)|\n+----------+-----------+\n|2023-01-01|        300|\n|2023-01-02|        200|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('date').agg(F.max('amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4cb754b-6ba8-41e2-ae01-9ccad0a4a7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 3: Fill Missing Cities With Default\n",
    "\n",
    "Replace null city values with 'Unknown'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f859fe-9a66-447e-9b11-51599d632f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n|customer_id|  city|\n+-----------+------+\n|          1|Dallas|\n|          2|  NULL|\n|          3|Austin|\n|          4|  NULL|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(customer_id=1, city='Dallas'),\n",
    "    Row(customer_id=2, city=None),\n",
    "    Row(customer_id=3, city='Austin'),\n",
    "    Row(customer_id=4, city=None)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7020361b-672c-4950-84c3-61f38e8599a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|customer_id|   city|\n+-----------+-------+\n|          1| Dallas|\n|          2|Unknown|\n|          3| Austin|\n|          4|Unknown|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.fillna(value='Unknown', subset=['city']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c0d3cf-b2c3-4bbd-8da3-775125f29d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 4: Compute Running Total by Customer\n",
    "\n",
    "Use a window function to compute cumulative sum of purchases per customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cdedf4-4a03-4ea2-b9a7-b26a7f3f9efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n|customer_id|      date|amount|\n+-----------+----------+------+\n|          1|2023-01-01|   100|\n|          1|2023-01-02|   200|\n|          2|2023-01-01|   300|\n|          2|2023-01-02|   400|\n+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(customer_id=1, date='2023-01-01', amount=100),\n",
    "    Row(customer_id=1, date='2023-01-02', amount=200),\n",
    "    Row(customer_id=2, date='2023-01-01', amount=300),\n",
    "    Row(customer_id=2, date='2023-01-02', amount=400)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da366f62-d954-487d-ab2c-6e8edfb5c001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>date</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>2023-01-01</td><td>100</td></tr><tr><td>1</td><td>2023-01-02</td><td>200</td></tr><tr><td>2</td><td>2023-01-01</td><td>300</td></tr><tr><td>2</td><td>2023-01-02</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2023-01-01",
         100
        ],
        [
         1,
         "2023-01-02",
         200
        ],
        [
         2,
         "2023-01-01",
         300
        ],
        [
         2,
         "2023-01-02",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f23e32e-7160-4aca-856a-25036650dd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+\n|customer_id|      date|amount|amount_cum|\n+-----------+----------+------+----------+\n|          1|2023-01-01|   100|       300|\n|          1|2023-01-02|   200|       300|\n|          2|2023-01-01|   300|       700|\n|          2|2023-01-02|   400|       700|\n+-----------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "w = Window.partitionBy('customer_id')\n",
    "cum_sum = df.withColumn('amount_cum', F.sum('amount').over(w))\n",
    "cum_sum.show()\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ace346d-4664-4fb1-97a0-096e5b8a955a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+\n|customer_id|      date|amount|amount_cum|\n+-----------+----------+------+----------+\n|          1|2023-01-01|   100|       300|\n|          1|2023-01-02|   200|       300|\n|          2|2023-01-01|   300|       700|\n|          2|2023-01-02|   400|       700|\n+-----------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e1e5654-4d44-4b41-9d60-23410f265e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 5: Average Sales Per Product\n",
    "\n",
    "Find average amount per product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d02031b-76b7-4307-9cb4-ba7181680e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|product|amount|\n+-------+------+\n|      A|   100|\n|      B|   200|\n|      A|   300|\n|      B|   400|\n+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(product='A', amount=100),\n",
    "    Row(product='B', amount=200),\n",
    "    Row(product='A', amount=300),\n",
    "    Row(product='B', amount=400)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fd4f04-c6e7-42fe-a304-8e0e96434ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n|product|avg(amount)|\n+-------+-----------+\n|      A|      200.0|\n|      B|      300.0|\n+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('product').agg(F.avg('amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07e1180-5038-4294-a31d-b38f9f1e11ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 6: Extract Year From Date\n",
    "\n",
    "Add a column to extract year from given date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a700273-bd76-4681-814a-7ecd99148fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n|customer|transaction_date|\n+--------+----------------+\n|    John|      2022-11-01|\n|   Alice|      2023-01-01|\n+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(customer='John', transaction_date='2022-11-01'),\n",
    "    Row(customer='Alice', transaction_date='2023-01-01')\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f32820b-cfd7-4e31-9027-f029dd4a51c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----+\n|customer|transaction_date|year|\n+--------+----------------+----+\n|    John|      2022-11-01|2022|\n|   Alice|      2023-01-01|2023|\n+--------+----------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df_withYear = df.withColumn('year',year(df['transaction_date'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "168cc890-f109-4ef6-b48b-e433c0e3ee10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 7: Join Product and Sales Data\n",
    "\n",
    "Join two DataFrames on product_id to get product names with amounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef0dad0-3b40-462f-849d-d9179f0430b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|product_id|product_name|\n+----------+------------+\n|         1|       Phone|\n|         2|      Tablet|\n+----------+------------+\n\n+----------+------+\n|product_id|amount|\n+----------+------+\n|         1|   500|\n|         2|   800|\n|         1|   200|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "products = [\n",
    "    Row(product_id=1, product_name='Phone'),\n",
    "    Row(product_id=2, product_name='Tablet')\n",
    "]\n",
    "sales = [\n",
    "    Row(product_id=1, amount=500),\n",
    "    Row(product_id=2, amount=800),\n",
    "    Row(product_id=1, amount=200)\n",
    "]\n",
    "df_products = spark.createDataFrame(products)\n",
    "df_sales = spark.createDataFrame(sales)\n",
    "df_products.show()\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e41ea3d-65d5-4893-98bb-209c5333a0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+\n|product_id|product_name|amount|\n+----------+------------+------+\n|         1|       Phone|   200|\n|         1|       Phone|   500|\n|         2|      Tablet|   800|\n+----------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_products.join(df_sales, on='product_id', how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ff690b-df42-4997-9176-c99a9a52e874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 8: Split Tags Into Rows\n",
    "\n",
    "Given a list of comma-separated tags, explode them into individual rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1929847a-a7fd-4e94-9376-0ae6cf6ccb3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n| id|        tags|\n+---+------------+\n|  1|   tech,news|\n|  2|sports,music|\n|  3|        food|\n+---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(id=1, tags='tech,news'),\n",
    "    Row(id=2, tags='sports,music'),\n",
    "    Row(id=3, tags='food')\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a04f5a-da71-409e-8f22-66e95d92373d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  tags|\n+---+------+\n|  1|  tech|\n|  1|  news|\n|  2|sports|\n|  2| music|\n|  3|  food|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode, split\n",
    "df.withColumn('tags',explode(split(df['tags'],','))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ecaa034-b178-4606-be7d-166d8856aa10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 9: Top-N Records Per Group\n",
    "\n",
    "For each category, return top 2 records based on score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d80521a-dc5d-4338-9db6-0853eb88bcd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n|category|name|score|\n+--------+----+-----+\n|       A|   x|   80|\n|       A|   y|   90|\n|       A|   z|   70|\n|       B|   p|   60|\n|       B|   q|   85|\n+--------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(category='A', name='x', score=80),\n",
    "    Row(category='A', name='y', score=90),\n",
    "    Row(category='A', name='z', score=70),\n",
    "    Row(category='B', name='p', score=60),\n",
    "    Row(category='B', name='q', score=85)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a478c8e9-0718-4923-9d31-c14a0c6fc854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n|category|name|score|\n+--------+----+-----+\n|       A|   y|   90|\n|       A|   x|   80|\n|       B|   q|   85|\n|       B|   p|   60|\n+--------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy('category').orderBy(F.desc('score'))\n",
    "df_ranked = df.withColumn(\"rank\", F.row_number().over(w))\n",
    "top_2 = df_ranked.filter(F.col('rank') <= 2)\n",
    "top_2 = top_2.drop('rank')\n",
    "top_2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa5f5dfa-fce9-4157-ab6c-be80d01c04e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Challenge 10: Null Safe Join\n",
    "\n",
    "Join two datasets where join key might have nulls, handle using null-safe join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250af3c2-52d6-4b1a-8a66-9055771c240d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n|  id| name|\n+----+-----+\n|   1| John|\n|NULL| Mike|\n|   2|Alice|\n+----+-----+\n\n+----+------+\n|  id|salary|\n+----+------+\n|   1|  5000|\n|NULL|  3000|\n+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    Row(id=1, name='John'),\n",
    "    Row(id=None, name='Mike'),\n",
    "    Row(id=2, name='Alice')\n",
    "]\n",
    "data2 = [\n",
    "    Row(id=1, salary=5000),\n",
    "    Row(id=None, salary=3000)\n",
    "]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df1.show()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5829aa-7b9b-4935-91dd-6956aff913bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+------+\n|  id| name|  id|salary|\n+----+-----+----+------+\n|   1| John|   1|  5000|\n|NULL| Mike|NULL|  3000|\n|   2|Alice|NULL|  NULL|\n+----+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1['id'].eqNullSafe(df2['id']),'outer').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Challenges | Naman",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}